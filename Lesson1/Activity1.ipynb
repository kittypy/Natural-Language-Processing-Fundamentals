{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Activity1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwJTCKi256cj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlcTxFsq39kq",
        "outputId": "73581c4f-11ca-4195-b884-7236af25b23f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install autocorrect\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/a8/1fc332535fc26db807fa48bdb54070355b83a36c797451c3d563bc190fa8/autocorrect-2.3.0.tar.gz (621kB)\n",
            "\r\u001b[K     |▌                               | 10kB 12.1MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 10.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 6.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 276kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 317kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 512kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 552kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 593kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 5.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.3.0-cp37-none-any.whl size=621587 sha256=0c712fe03be41be1086227a1fe335707b5ec3b4c04760298f3be5e1fd7f47d45\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/1c/30/6b0199afbd20eef5959f5eaa0ead86aeef84391740482b2279\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHVC2y6b4XSw",
        "outputId": "9c15c099-ae36-4bac-cbe3-dc992c906862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeC13tqf49IR",
        "outputId": "75e2a0e7-bb52-441c-d231-e21ab0ddbaf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! ls /content/drive/MyDrive/data/nlp-fundamental/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_ch1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMjarVPC3fJl",
        "outputId": "17fdffa6-dac7-4a38-9876-f07b791349e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from autocorrect import Speller\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxDifRUg5jyT",
        "outputId": "6cfa8b6f-2329-43f6-8b0e-3c45112c29bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content/drive/MyDrive/data/"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nlp-fundamental\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaU4JMeI8ioe"
      },
      "source": [
        "Speller??"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeUNErw33fJm"
      },
      "source": [
        "data_root = '/content/drive/MyDrive/data/nlp-fundamental'\n",
        "sentence = open(f\"{data_root}/data_ch1/file.txt\", \"r\").read()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsL3lua27cZW",
        "outputId": "78013f20-25b0-4d06-ca41-3f19ab1af245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "string.whitespace, string.punctuation, string.hexdigits"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' \\t\\n\\r\\x0b\\x0c',\n",
              " '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~',\n",
              " '0123456789abcdefABCDEF')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FmGPdV_5tMq",
        "outputId": "a5f3e940-874e-4519-f326-c2d6dc713b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens1 = sentence.split(' ')\n",
        "tokens2 = sentence.split() # any whitespace including \\n\n",
        "\n",
        "[set(tokens1)-(set(tokens2)), set(tokens2)-(set(tokens1))]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'now.\\n'}, {'now.'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDuGwvJE3fJn",
        "outputId": "d9c02060-ce1d-4f1a-c8e5-cbbdf3ffd959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words = word_tokenize(sentence)\n",
        "print(words[0:20])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learnning', 'how', 'to', 'pracess', 'Natueral', 'Language', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8v0XuI_6Mlm",
        "outputId": "60d1cad9-4574-479d-d24a-861aeb566767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "set(words).symmetric_difference(set(tokens))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{',',\n",
              " '.',\n",
              " 'Gunning',\n",
              " 'Gunning,',\n",
              " 'NLP.',\n",
              " 'it',\n",
              " 'it,',\n",
              " 'it.',\n",
              " 'now',\n",
              " 'now.\\n',\n",
              " 'prajects',\n",
              " 'prajects.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHRB_QMy3fJn",
        "outputId": "f970f693-25d9-4498-cb4c-232dad396f82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "corrected_sentence = \"\"\n",
        "corrected_word_list = []\n",
        "speller = Speller()\n",
        "for wd in words:\n",
        "    if wd not in string.punctuation:\n",
        "        wd_c = speller.autocorrect_word(wd)\n",
        "        if wd_c != wd:\n",
        "            print(wd+\" has been corrected to: \"+wd_c)\n",
        "            corrected_sentence = corrected_sentence+\" \"+wd_c\n",
        "            corrected_word_list.append(wd_c)\n",
        "        else:\n",
        "            corrected_sentence = corrected_sentence+\" \"+wd\n",
        "            corrected_word_list.append(wd)\n",
        "    else:\n",
        "        corrected_sentence = corrected_sentence + wd\n",
        "        corrected_word_list.append(wd)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sohom has been corrected to: Show\n",
            "Ghosh has been corrected to: Ghost\n",
            "Dwight has been corrected to: Right\n",
            "Gunning has been corrected to: Running\n",
            "learnning has been corrected to: learning\n",
            "pracess has been corrected to: process\n",
            "Natueral has been corrected to: Natural\n",
            "NLP has been corrected to: LP\n",
            "NLP has been corrected to: LP\n",
            "prajects has been corrected to: projects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvYGaS0X3fJo",
        "outputId": "473e01e2-2690-49cf-e00b-56a784b6441c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "corrected_sentence"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' In this book authored by Show Ghost and Right Running, we shall learning how to process Natural Language and extract insights from it. The first four chapter will introduce you to the basics of LP. Later chapters will describe how to deal with complex LP projects. If you want to get early access of it, you should book your order now.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ53N8YQ3fJo",
        "outputId": "ebabdda8-aeb6-499c-dde2-52f05195105e"
      },
      "source": [
        "print(corrected_word_list[0:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'this', 'book', 'authored', 'by', 'Soho', 'Ghost', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learning', 'how', 'to', 'process', 'Natural', 'Language', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdJj801H3fJp",
        "outputId": "0e167917-d0a7-470b-d107-4f6336147cab"
      },
      "source": [
        "print(nltk.pos_tag(corrected_word_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('In', 'IN'), ('this', 'DT'), ('book', 'NN'), ('authored', 'VBN'), ('by', 'IN'), ('Soho', 'NNP'), ('Ghost', 'NNP'), ('and', 'CC'), ('Dwight', 'NNP'), ('Gunning', 'NNP'), (',', ','), ('we', 'PRP'), ('shall', 'MD'), ('learning', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('process', 'VB'), ('Natural', 'NNP'), ('Language', 'NNP'), ('and', 'CC'), ('extract', 'JJ'), ('insights', 'NNS'), ('from', 'IN'), ('it', 'PRP'), ('.', '.'), ('The', 'DT'), ('first', 'JJ'), ('four', 'CD'), ('chapter', 'NN'), ('will', 'MD'), ('introduce', 'VB'), ('you', 'PRP'), ('to', 'TO'), ('the', 'DT'), ('basics', 'NNS'), ('of', 'IN'), ('NLP', 'NNP'), ('.', '.'), ('Later', 'NNP'), ('chapters', 'NNS'), ('will', 'MD'), ('describe', 'VB'), ('how', 'WRB'), ('to', 'TO'), ('deal', 'VB'), ('with', 'IN'), ('complex', 'JJ'), ('NLP', 'NNP'), ('projects', 'NNS'), ('.', '.'), ('If', 'IN'), ('you', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('get', 'VB'), ('early', 'JJ'), ('access', 'NN'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('you', 'PRP'), ('should', 'MD'), ('book', 'NN'), ('your', 'PRP$'), ('order', 'NN'), ('now', 'RB'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH-yE7fE3fJp",
        "outputId": "bee95dd3-f14b-443d-e706-2ba3bab8f2d6"
      },
      "source": [
        "stop_words = stopwords.words('English')\n",
        "corrected_word_list_without_stopwords = []\n",
        "for wd in corrected_word_list:\n",
        "    if wd not in stop_words:\n",
        "        corrected_word_list_without_stopwords.append(wd)\n",
        "corrected_word_list_without_stopwords[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'book',\n",
              " 'authored',\n",
              " 'Soho',\n",
              " 'Ghost',\n",
              " 'Dwight',\n",
              " 'Gunning',\n",
              " ',',\n",
              " 'shall',\n",
              " 'learning',\n",
              " 'process',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'extract',\n",
              " 'insights',\n",
              " '.',\n",
              " 'The',\n",
              " 'first',\n",
              " 'four',\n",
              " 'chapter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7-cgh1g3fJp",
        "outputId": "6c767c89-8669-41b8-ec13-4b79a975bc98"
      },
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "corrected_word_list_without_stopwords_stemmed = []\n",
        "for wd in corrected_word_list_without_stopwords:\n",
        "    corrected_word_list_without_stopwords_stemmed.append(stemmer.stem(wd))\n",
        "corrected_word_list_without_stopwords_stemmed[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'book',\n",
              " 'author',\n",
              " 'soho',\n",
              " 'ghost',\n",
              " 'dwight',\n",
              " 'gun',\n",
              " ',',\n",
              " 'shall',\n",
              " 'learn',\n",
              " 'process',\n",
              " 'natur',\n",
              " 'languag',\n",
              " 'extract',\n",
              " 'insight',\n",
              " '.',\n",
              " 'the',\n",
              " 'first',\n",
              " 'four',\n",
              " 'chapter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "580LQW0v3fJq",
        "outputId": "af3e16f5-3bcb-44ae-a3e9-8257cf90981b"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "corrected_word_list_without_stopwords_lemmatized = []\n",
        "for wd in corrected_word_list_without_stopwords:\n",
        "    corrected_word_list_without_stopwords_lemmatized.append(lemmatizer.lemmatize(wd))\n",
        "corrected_word_list_without_stopwords_lemmatized[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'book',\n",
              " 'authored',\n",
              " 'Soho',\n",
              " 'Ghost',\n",
              " 'Dwight',\n",
              " 'Gunning',\n",
              " ',',\n",
              " 'shall',\n",
              " 'learning',\n",
              " 'process',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'extract',\n",
              " 'insight',\n",
              " '.',\n",
              " 'The',\n",
              " 'first',\n",
              " 'four',\n",
              " 'chapter']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtiQykHu3fJq",
        "outputId": "4054b42e-ed62-473a-82aa-736b40ec0862"
      },
      "source": [
        "print(sent_tokenize(corrected_sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' In this book authored by Soho Ghost and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it.', 'The first four chapter will introduce you to the basics of NLP.', 'Later chapters will describe how to deal with complex NLP projects.', 'If you want to get early access of it, you should book your order now.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztjpuQCf3fJq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}